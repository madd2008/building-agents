{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd0bcb",
   "metadata": {},
   "source": [
    "# [STARTER] Udaplay Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325b035",
   "metadata": {},
   "source": [
    "## Part 02 - Agent\n",
    "\n",
    "In this part of the project, you'll use your VectorDB to be part of your Agent as a tool.\n",
    "\n",
    "You're building UdaPlay, an AI Research Agent for the video game industry. The agent will:\n",
    "1. Answer questions using internal knowledge (RAG)\n",
    "2. Search the web when needed\n",
    "3. Maintain conversation state\n",
    "4. Return structured outputs\n",
    "5. Store useful information for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42de90",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only needed for Udacity workspace\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Check if 'pysqlite3' is available before importing\n",
    "if importlib.util.find_spec(\"pysqlite3\") is not None:\n",
    "    import pysqlite3\n",
    "    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import the necessary libs\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LLM + HTTP\n",
    "import openai\n",
    "import requests\n",
    "\n",
    "# Optional Tavily SDK fallback\n",
    "try:\n",
    "    from tavily import TavilyClient\n",
    "    _HAS_TAVILY_SDK = True\n",
    "except Exception:\n",
    "    _HAS_TAVILY_SDK = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e465d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "CHROMA_OPENAI_API_KEY = os.getenv(\"CHROMA_OPENAI_API_KEY\") or OPENAI_API_KEY\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-3.5-turbo\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY is required in .env\")\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce364221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Chroma client & collection\n",
    "# -------------------------\n",
    "DB_PATH = os.getenv(\"CHROMA_PERSIST_PATH\", \"chromadb\")  # persistent folder\n",
    "chroma_client = chromadb.PersistentClient(path=DB_PATH)\n",
    "\n",
    "# Use OpenAI embedding function. Make sure embedding API key is available.\n",
    "embedding_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=CHROMA_OPENAI_API_KEY,\n",
    "    model_name=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "COLLECTION_NAME = \"udaplay\"\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    embedding_function=embedding_fn\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Simple Memory\n",
    "# -------------------------\n",
    "MEMORY_FILE = \"memory.jsonl\"\n",
    "\n",
    "\n",
    "def append_memory(record: Dict[str, Any]) -> None:\n",
    "    \"\"\"Append a JSON line to memory.\"\"\"\n",
    "    with open(MEMORY_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de4729",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab2dac",
   "metadata": {},
   "source": [
    "Build at least 3 tools:\n",
    "- retrieve_game: To search the vector DB\n",
    "- evaluate_retrieval: To assess the retrieval performance\n",
    "- game_web_search: If no good, search the web\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f14cd",
   "metadata": {},
   "source": [
    "#### Retrieve Game Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create retrieve_game tool\n",
    "# -------------------------\n",
    "# Tools\n",
    "# -------------------------\n",
    "\n",
    "def retrieve_game(query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Semantic search over the 'udaplay' Chroma collection.\n",
    "    Returns a list of hits with 'id', 'document', 'metadata', 'distance'.\n",
    "    \"\"\"\n",
    "    if collection is None:\n",
    "        raise RuntimeError(\"Chroma collection not available\")\n",
    "\n",
    "    # Chroma returns lists nested for batch queries (we query single)\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\", \"ids\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    hits = []\n",
    "    if results and \"ids\" in results and len(results[\"ids\"]) > 0:\n",
    "        for i in range(len(results[\"ids\"][0])):\n",
    "            hit = {\n",
    "                \"id\": results[\"ids\"][0][i],\n",
    "                \"document\": results[\"documents\"][0][i] if results.get(\"documents\") else None,\n",
    "                \"metadata\": results[\"metadatas\"][0][i] if results.get(\"metadatas\") else None,\n",
    "                \"distance\": results[\"distances\"][0][i] if results.get(\"distances\") else None,\n",
    "            }\n",
    "            hits.append(hit)\n",
    "    return hits\n",
    "\n",
    "\n",
    "def _call_openai_judge(prompt: str, model: str = OPENAI_MODEL, temperature: float = 0.0) -> str:\n",
    "    \"\"\"Small wrapper to call OpenAI chat completion and return assistant text.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, precise evaluator.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    resp = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return resp.choices[0].message[\"content\"].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910dc945",
   "metadata": {},
   "source": [
    "#### Evaluate Retrieval Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create evaluate_retrieval tool\n",
    "def evaluate_retrieval(question: str, retrieved_docs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Uses an LLM to evaluate if retrieved documents are sufficient to answer the question.\n",
    "    Returns a dict: { useful: bool, description: str }\n",
    "    The LLM is instructed to return JSON only.\n",
    "    \"\"\"\n",
    "    docs_summary = \"\\n\\n\".join(\n",
    "        [f\"ID: {d['id']}\\nDoc: {d['document']}\\nMeta: {json.dumps(d['metadata'], ensure_ascii=False)}\" for d in retrieved_docs]\n",
    "    ) or \"(no documents)\"\n",
    "\n",
    "    prompt = (\n",
    "        \"Your task is to evaluate whether the following retrieved documents are sufficient to answer the user question.\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        \"Retrieved documents:\\n\"\n",
    "        f\"{docs_summary}\\n\\n\"\n",
    "        \"Return a JSON object with fields:\\n\"\n",
    "        \" - useful: true or false (are these documents enough to provide a correct, evidence-backed answer?)\\n\"\n",
    "        \" - description: a short explanation of your judgement and what is missing (if anything)\\n\\n\"\n",
    "        \"IMPORTANT: Return only valid JSON (no extra commentary).\"\n",
    "    )\n",
    "\n",
    "    assistant_text = _call_openai_judge(prompt)\n",
    "    # Try to parse JSON robustly:\n",
    "    try:\n",
    "        parsed = json.loads(assistant_text)\n",
    "    except Exception:\n",
    "        # fallback: wrap text in description if not JSON\n",
    "        parsed = {\"useful\": False, \"description\": assistant_text}\n",
    "\n",
    "    # Normalize\n",
    "    if \"useful\" not in parsed:\n",
    "        parsed[\"useful\"] = bool(parsed.get(\"useful\", False))\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7935a26",
   "metadata": {},
   "source": [
    "#### Game Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad698aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create game_web_search tool\n",
    "def game_web_search(question: str, max_results: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Uses Tavily to do a web search when the Vector DB is insufficient.\n",
    "    Returns the JSON Tavily reply (answer + results).\n",
    "    \"\"\"\n",
    "    if not TAVILY_API_KEY:\n",
    "        raise RuntimeError(\"TAVILY_API_KEY not set in environment\")\n",
    "\n",
    "    body = {\n",
    "        \"query\": question,\n",
    "        \"include_answer\": True,\n",
    "        \"include_raw_content\": False,\n",
    "        \"max_results\": max_results,\n",
    "        \"search_depth\": \"basic\"\n",
    "    }\n",
    "\n",
    "    # Prefer tavily SDK if installed\n",
    "    if _HAS_TAVILY_SDK:\n",
    "        try:\n",
    "            client = TavilyClient(api_key=TAVILY_API_KEY)\n",
    "            resp = client.search(question, max_results=max_results, include_answer=True)\n",
    "            return resp\n",
    "        except Exception as e:\n",
    "            # fallback to REST\n",
    "            print(\"Tavily SDK failed, falling back to REST:\", e)\n",
    "\n",
    "    # REST fallback\n",
    "    url = \"https://api.tavily.com/search\"\n",
    "    headers = {\"Authorization\": f\"Bearer {TAVILY_API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "    r = requests.post(url, headers=headers, json=body, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def compose_answer_from_web(self, question: str, web_resp: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Compose answer using web search (Tavily) plus optionally retrieved docs.\n",
    "        Expects Tavily-style response (contains 'answer' and 'results').\n",
    "        \"\"\"\n",
    "        # If tavily included an answer, use it as starting point\n",
    "        tavily_answer = web_resp.get(\"answer\") if isinstance(web_resp, dict) else None\n",
    "        results = web_resp.get(\"results\", []) if isinstance(web_resp, dict) else []\n",
    "\n",
    "        # Build prompt with top results\n",
    "        results_text = \"\\n\\n\".join([f\"{r.get('title','')}\\n{r.get('url','')}\\n{r.get('content','')[:500]}\" for r in results[:5]])\n",
    "        prompt = (\n",
    "            \"You are UdaPlay. Using the following web search answer + top results, produce a structured JSON:\\n\"\n",
    "            \" - answer: short answer with citations (format: [source-index])\\n\"\n",
    "            \" - sources: list of objects {idx: int, title: str, url: str}\\n\"\n",
    "            \" - confidence: low/medium/high\\n\\n\"\n",
    "            f\"Original question: {question}\\n\\n\"\n",
    "            f\"Tavily quick answer: {tavily_answer}\\n\\nTop results:\\n{results_text}\\n\\n\"\n",
    "            \"Return only valid JSON.\"\n",
    "        )\n",
    "        assistant_text = _call_openai_judge(prompt, temperature=0.0)\n",
    "        try:\n",
    "            parsed = json.loads(assistant_text)\n",
    "        except Exception:\n",
    "            # fallback basic structure\n",
    "            parsed = {\n",
    "                \"answer\": tavily_answer or \"No web answer available.\",\n",
    "                \"sources\": [{\"idx\": i+1, \"title\": r.get(\"title\"), \"url\": r.get(\"url\")} for i, r in enumerate(results[:5])],\n",
    "                \"confidence\": \"medium\" if tavily_answer else \"low\"\n",
    "            }\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df844b3b",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c56281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your Agent abstraction using StateMachine\n",
    "# -------------------------\n",
    "# Answer composer (Agent)\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class AgentResult:\n",
    "    question: str\n",
    "    answer: str\n",
    "    sources: List[Dict[str, Any]]\n",
    "    used_tools: List[str]\n",
    "    evaluation: Dict[str, Any]\n",
    "\n",
    "\n",
    "class UdaPlayAgent:\n",
    "    def __init__(self):\n",
    "        self.memory = []  # short session memory; persisted via append_memory\n",
    "        self.tools = {\n",
    "            \"retrieve_game\": retrieve_game,\n",
    "            \"evaluate_retrieval\": evaluate_retrieval,\n",
    "            \"game_web_search\": game_web_search\n",
    "        }\n",
    "\n",
    "    def compose_answer_from_docs(self, question: str, docs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Ask the LLM to produce a structured answer using the retrieved docs as context.\n",
    "        Returns a dict with keys 'answer' and 'sources' (ids + summary)\n",
    "        \"\"\"\n",
    "        docs_context = \"\\n\\n\".join([f\"ID: {d['id']}\\nText: {d['document']}\\nMeta: {json.dumps(d['metadata'], ensure_ascii=False)}\"\n",
    "                                     for d in docs])\n",
    "        prompt = (\n",
    "            \"You are UdaPlay: an AI research agent for the video game industry.\\n\"\n",
    "            \"Use ONLY the documents below as your evidence to answer the user's question. \"\n",
    "            \"Produce a JSON object with keys:\\n\"\n",
    "            \" - answer: concise answer to the question\\n\"\n",
    "            \" - sources: list of objects {id: <doc id>, excerpt: <one-sentence excerpt>}\\n\"\n",
    "            \" - confidence: low/medium/high\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"Documents:\\n{docs_context}\\n\\n\"\n",
    "            \"Return ONLY valid JSON.\"\n",
    "        )\n",
    "        assistant_text = _call_openai_judge(prompt, temperature=0.0)\n",
    "        try:\n",
    "            parsed = json.loads(assistant_text)\n",
    "        except Exception:\n",
    "            parsed = {\"answer\": assistant_text, \"sources\": [{\"id\": d[\"id\"], \"excerpt\": (d[\"document\"] or \"\")[:200]} for d in docs], \"confidence\": \"low\"}\n",
    "        return parsed\n",
    "\n",
    "    def compose_answer_from_web(self, question: str, web_resp: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Compose answer using web search (Tavily) plus optionally retrieved docs.\n",
    "        Expects Tavily-style response (contains 'answer' and 'results').\n",
    "        \"\"\"\n",
    "        # If tavily included an answer, use it as starting point\n",
    "        tavily_answer = web_resp.get(\"answer\") if isinstance(web_resp, dict) else None\n",
    "        results = web_resp.get(\"results\", []) if isinstance(web_resp, dict) else []\n",
    "\n",
    "        # Build prompt with top results\n",
    "        results_text = \"\\n\\n\".join([f\"{r.get('title','')}\\n{r.get('url','')}\\n{r.get('content','')[:500]}\" for r in results[:5]])\n",
    "        prompt = (\n",
    "            \"You are UdaPlay. Using the following web search answer + top results, produce a structured JSON:\\n\"\n",
    "            \" - answer: short answer with citations (format: [source-index])\\n\"\n",
    "            \" - sources: list of objects {idx: int, title: str, url: str}\\n\"\n",
    "            \" - confidence: low/medium/high\\n\\n\"\n",
    "            f\"Original question: {question}\\n\\n\"\n",
    "            f\"Tavily quick answer: {tavily_answer}\\n\\nTop results:\\n{results_text}\\n\\n\"\n",
    "            \"Return only valid JSON.\"\n",
    "        )\n",
    "        assistant_text = _call_openai_judge(prompt, temperature=0.0)\n",
    "        try:\n",
    "            parsed = json.loads(assistant_text)\n",
    "        except Exception:\n",
    "            # fallback basic structure\n",
    "            parsed = {\n",
    "                \"answer\": tavily_answer or \"No web answer available.\",\n",
    "                \"sources\": [{\"idx\": i+1, \"title\": r.get(\"title\"), \"url\": r.get(\"url\")} for i, r in enumerate(results[:5])],\n",
    "                \"confidence\": \"medium\" if tavily_answer else \"low\"\n",
    "            }\n",
    "        return parsed\n",
    "\n",
    "    def run(self, question: str, k: int = 3) -> AgentResult:\n",
    "        \"\"\"\n",
    "        Top-level orchestration:\n",
    "         - retrieve -> evaluate -> (if needed web search) -> finalize answer -> store memory\n",
    "        \"\"\"\n",
    "        used_tools = []\n",
    "        retrieved = []\n",
    "        try:\n",
    "            retrieved = self.tools[\"retrieve_game\"](question, k=k)\n",
    "            used_tools.append(\"retrieve_game\")\n",
    "        except Exception as e:\n",
    "            print(\"retrieve_game error:\", e)\n",
    "            retrieved = []\n",
    "\n",
    "        evaluation = {\"useful\": False, \"description\": \"no evaluation performed\"}\n",
    "        try:\n",
    "            evaluation = self.tools[\"evaluate_retrieval\"](question, retrieved)\n",
    "            used_tools.append(\"evaluate_retrieval\")\n",
    "        except Exception as e:\n",
    "            print(\"evaluate_retrieval error:\", e)\n",
    "\n",
    "        final_answer = {}\n",
    "        sources = []\n",
    "\n",
    "        if evaluation.get(\"useful\"):\n",
    "            composed = self.compose_answer_from_docs(question, retrieved)\n",
    "            final_answer = composed.get(\"answer\") if isinstance(composed, dict) else str(composed)\n",
    "            sources = composed.get(\"sources\", [])\n",
    "            used_tools.append(\"compose_from_docs\")\n",
    "        else:\n",
    "            # fallback to web search\n",
    "            try:\n",
    "                web_resp = self.tools[\"game_web_search\"](question, max_results=5)\n",
    "                used_tools.append(\"game_web_search\")\n",
    "                composed = self.compose_answer_from_web(question, web_resp)\n",
    "                final_answer = composed.get(\"answer\")\n",
    "                sources = composed.get(\"sources\", [])\n",
    "            except Exception as e:\n",
    "                print(\"game_web_search error:\", e)\n",
    "                # As last resort, ask LLM to answer from retrieved docs even if 'not useful'\n",
    "                composed = self.compose_answer_from_docs(question, retrieved)\n",
    "                final_answer = composed.get(\"answer\")\n",
    "                sources = composed.get(\"sources\", [])\n",
    "\n",
    "        result = AgentResult(\n",
    "            question=question,\n",
    "            answer=final_answer,\n",
    "            sources=sources,\n",
    "            used_tools=used_tools,\n",
    "            evaluation=evaluation\n",
    "        )\n",
    "\n",
    "        # Persist a short memory record\n",
    "        mem_rec = {\n",
    "            \"timestamp\": time.time(),\n",
    "            \"question\": question,\n",
    "            \"answer\": final_answer,\n",
    "            \"used_tools\": used_tools,\n",
    "            \"evaluation\": evaluation\n",
    "        }\n",
    "        append_memory(mem_rec)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec23893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Invoke your agent\n",
    "# -------------------------\n",
    "# Demo run for sample questions\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    agent = UdaPlayAgent()\n",
    "\n",
    "    sample_questions = [\n",
    "        \"When were Pokémon Gold and Silver released?\",\n",
    "        \"Which one was the first 3D platformer Mario game?\",\n",
    "        \"Was Mortal Kombat X released for PlayStation 5?\"\n",
    "    ]\n",
    "\n",
    "    for q in sample_questions:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"Question:\", q)\n",
    "        try:\n",
    "            res = agent.run(q, k=3)\n",
    "            print(\"Answer (structured):\")\n",
    "            print(json.dumps({\n",
    "                \"answer\": res.answer,\n",
    "                \"sources\": res.sources,\n",
    "                \"evaluation\": res.evaluation,\n",
    "                \"used_tools\": res.used_tools\n",
    "            }, ensure_ascii=False, indent=2))\n",
    "        except Exception as exc:\n",
    "            print(\"Agent run failed:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a55081",
   "metadata": {},
   "source": [
    "### (Optional) Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update your agent with long-term memory\n",
    "# TODO: Convert the agent to be a state machine, with the tools being pre-defined nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
